{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe54199",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064cff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Piotr Durniat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace83175",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cdf68-8b02-44ae-8209-7bf9cf0fef25",
   "metadata": {},
   "source": [
    "# SimCLR\n",
    "W poprzednim zeszycie rozważana była estymacja rozkładu z wykorzystaniem NCE. InfoNCE jest jego rozszerzeniem do problemu wieloklasowego i zostało wykorzystane w licznych metodach kontrastowego uczenia samonadzorowanego. W niniejszym zeszycie skupimy się na metodzie `SimCLR` [(Chen et al., 2020)](https://arxiv.org/abs/2002.05709). **Należy się dokładnie zapoznać z ideą i zasadą działania SimCLR** (wykorzystując materiały z wykładu oraz publikacje, bądź inne materiały dostępne w sieci). \n",
    "\n",
    "\n",
    "W niniejszym zeszycie należy wykonać zadania z zakresu implementacji metody `SimCLR` oraz badania jej hiperparametrów. Model oraz trenowanie zaimplementowano z wykorzystaniem `pytorch_lightning`. Żeby zrozumieć dobrze implementacje, należy zapoznać się z klasą bazową `SSLBase` i ewaluacją na zadaniu docelowym, która została w tej klasie zaimplementowana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0035b4d-d7f1-4a68-b519-cb6415f4c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbe4b31-c797-49d6-9932-ff9a06c98b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from lightning_fabric import seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning import Trainer\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from src.data import VisionDatamodule\n",
    "from src.ssl_base import SSLBase\n",
    "from src.networks import SmallConvnet, MLP\n",
    "from src.augmentations import get_default_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d01b43-f28b-4b7b-98aa-898713849009",
   "metadata": {},
   "source": [
    "# Zbiór danych\n",
    "Do uczenia `SimCLR` wykorzystamy wypróbkowany podzbiór zbioru MNIST. Warto zaznaczyć, że jest to wybór podyktowany tylko i wyłącznie ograniczeniami w zasobach obliczeniowych. W rzeczywistości, aby otrzymać konkluzywne i rzetelne wyniki, należałoby skorzystać co najmniej ze zbioru `CIFAR-10` oraz znacznie większego modelu kodera, np. `ResNet`. Jednak do celów dydaktycznych skorzystamy z mniejszego zbioru oraz odpowiednio mniejszego kodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1600bfc0-49b1-4d1b-911d-5ff61557ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "DATA_DIR = \"./data\"\n",
    "OUT_DIR = \"./data/output/sim_clr\"\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "NUM_SAMPLES_PER_CLASS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009daf18-03c7-4f79-b0f1-54d376a3aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = VisionDatamodule(\n",
    "    root_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    num_samples_per_class=NUM_SAMPLES_PER_CLASS,\n",
    ")\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202c2ee-eb8c-4763-8262-d24666965a81",
   "metadata": {},
   "source": [
    "# Zadanie 2.1 Implementacja modelu SimCLR (2.0 pkt)\n",
    "W poniższej komórce znajduje się częściowa implementacja modelu `SimCLR`. Uzupełnij brakujące implementacje funkcji:\n",
    "* `forward` (1.0 pkt) - implementuje przejście w przód modelu (augmentacje do dwóch widoków, forward widoków przez `encoder` oraz `projector`), zwraca parę embeddingów.\n",
    "* `info_nce_loss` (1.0 pkt) - funkcja straty modelu `SimCLR`:\n",
    "$$\n",
    "\\ell_{i, j}=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(z_i, z_j\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N} \\mathbb{1}_{[k \\neq i]} \\exp \\left(\\operatorname{sim}\\left(z_i, z_k\\right) / \\tau\\right)},\n",
    "$$\n",
    "gdzie $\\operatorname{sim}$ - podobieńśtwo cosinusowe, $\\tau$ - parametr temperatury.\n",
    "\n",
    "Ponadto:\n",
    "* Wykorzystuj metody z torch'a, unikaj pythonowych pętli.\n",
    "* Przeanalizuj dokładnie pozostałe elementy implementacji.\n",
    "* Uruchom uczenie i przeanalizuj wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53e1f28-c6a9-4c4c-add7-59301a131f1e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f127c13c858ec8a4f14d59f420e53666",
     "grade": true,
     "grade_id": "simclr-implementation",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SimCLRModel(SSLBase):\n",
    "\n",
    "    temp: float\n",
    "    encoder: nn.Module\n",
    "    decoder: nn.Module\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        temp: float,\n",
    "        out_channels: int = 10,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "\n",
    "        # Initialize online network\n",
    "        self.encoder = SmallConvnet()\n",
    "        self.projector = MLP(84, 84, 84, plain_last=True)\n",
    "\n",
    "        self.aug_1 = get_default_aug()\n",
    "        self.aug_2 = get_default_aug()\n",
    "\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        # TU WPISZ KOD\n",
    "\n",
    "        z_i = self.projector(self.encoder(self.aug_1(x)))\n",
    "        z_j = self.projector(self.encoder(self.aug_2(x)))\n",
    "        return z_i, z_j\n",
    "\n",
    "    def forward_repr(self, x: Tensor) -> Tensor:\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        z_i, z_j = self.forward(x)\n",
    "        loss = self.info_nce_loss(z_i=z_i, z_j=z_j)\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def info_nce_loss(self, z_i: Tensor, z_j: Tensor) -> Tensor:\n",
    "\n",
    "        # z_i and z_j are [512, 84]\n",
    "        batch_size = z_i.shape[0]\n",
    "\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = F.cosine_similarity(\n",
    "            representations.unsqueeze(1), representations.unsqueeze(0), dim=2\n",
    "        )\n",
    "\n",
    "        # Compute l_{i,j} only for positives paris (diagonal)\n",
    "        sim_ij = torch.diag(similarity_matrix, batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "        nominator = torch.exp(positives / self.temp)\n",
    "\n",
    "        negatives_mask = (\n",
    "            ~torch.eye(batch_size * 2, batch_size * 2, dtype=bool).to(z_i.device)\n",
    "        ).float()\n",
    "\n",
    "        denominator = negatives_mask * torch.exp(similarity_matrix / self.temp)\n",
    "\n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * batch_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95924778-012e-416e-b5c8-65c982f9aac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-11a6eceb932cb676\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-11a6eceb932cb676\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./data/output/sim_clr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5100419f-7cd7-44b9-86f0-a058848d714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | encoder   | SmallConvnet | 43.6 K\n",
      "1 | projector | MLP          | 14.4 K\n",
      "2 | aug_1     | Sequential   | 0     \n",
      "3 | aug_2     | Sequential   | 0     \n",
      "-------------------------------------------\n",
      "58.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "58.0 K    Total params\n",
      "0.232     Total estimated model params size (MB)\n",
      "/home/piotr/projects/ai/ur-l/l03-sr-17-piotrdurniat/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/piotr/projects/ai/ur-l/l03-sr-17-piotrdurniat/venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/home/piotr/projects/ai/ur-l/l03-sr-17-piotrdurniat/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5/5 [00:11<00:00,  0.43it/s, v_num=7, train/loss=0.892]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5/5 [00:11<00:00,  0.43it/s, v_num=7, train/loss=0.892]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/projects/ai/ur-l/l03-sr-17-piotrdurniat/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 20/20 [00:02<00:00,  9.83it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test/accuracy         0.7824334502220154\n",
      "         test/f1            0.7768892645835876\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test/accuracy': 0.7824334502220154, 'test/f1': 0.7768892645835876}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "TEMPERATURE = 0.07\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 75\n",
    "# EPOCHS = 5\n",
    "\n",
    "ACCELERATOR = \"cpu\"  # change to CUDA, if want to train on GPU\n",
    "\n",
    "seed_everything(42)\n",
    "model = SimCLRModel(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    temp=TEMPERATURE,\n",
    ")\n",
    "logger = TensorBoardLogger(save_dir=OUT_DIR, default_hp_metric=False)\n",
    "trainer = Trainer(\n",
    "    default_root_dir=OUT_DIR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=logger,\n",
    "    accelerator=ACCELERATOR,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066f1bf-b7c8-42a5-ae01-91210e729b68",
   "metadata": {},
   "source": [
    "# Zadanie 2.2 Badanie `batch_size` (1 pkt)\n",
    "Jako jedną z głównych wad metod kontrastowych podaje się wymaganie na stosowanie dużych batch'y. W tym zadaniu:\n",
    "* Sprawdź co najmniej 3 wielkości batch'a\n",
    "* Idelanie będzie wykonać 3-5 powtórzeń dla każdej wartości parametru, ale jeśli ograniczają Cię zasoby wystarczy 1\n",
    "* Z każdym kolejnym uruchominiem uczenia ustawiaj na nowo seed\n",
    "* Wynik na zbiorze testowym wylicz metodą `trainer.test(...)`, a na podstawie wyników sporządź wykres\n",
    "* Co zaobserwowałeś? Wyjaśnij otrzymaną zależność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e738542-3e4d-42e3-8e36-54705924a54b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4849a035bad3c55acfbbb099925cd25",
     "grade": true,
     "grade_id": "simclr-batch-size",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TU WPISZ KOD\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

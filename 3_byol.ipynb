{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9790f0c",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466003a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a54e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cdf68-8b02-44ae-8209-7bf9cf0fef25",
   "metadata": {},
   "source": [
    "# BYOL\n",
    "W niniejszym zeszycie skupimy się na metodzie *Bootsrap Your Own Latent* (BYOL) [(Grill et al., 2020)](https://arxiv.org/abs/2006.07733). **Należy się dokładnie zapoznać z ideą i zasadą działania BYOL** (wykorzystując materiały z wykładu oraz publikacje, bądź inne materiały dostępne w sieci). \n",
    "\n",
    "W niniejszym zeszycie należy wykonać zadania z zakresu implementacji metody BYOL oraz badania jej hiperparametrów. Model oraz trenowanie zaimplementowano z wykorzystaniem `pytorch_lightning`. Żeby zrozumieć dobrze implementacje, należy zapoznać się z klasą bazową `SSLBase` i ewaluacją na zadaniu docelowym, która została w tej klasie zaimplementowana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0035b4d-d7f1-4a68-b519-cb6415f4c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe4b31-c797-49d6-9932-ff9a06c98b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from lightning_fabric import seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning import Trainer\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from src.data import VisionDatamodule\n",
    "from src.ssl_base import SSLBase\n",
    "from src.networks import SmallConvnet, MLP\n",
    "from src.augmentations import get_default_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d01b43-f28b-4b7b-98aa-898713849009",
   "metadata": {},
   "source": [
    "# Zbiór danych\n",
    "Do uczenia BYOL wykorzystamy wypróbkowany podzbiór zbioru MNIST. Warto zaznaczyć, że jest to wybór podyktowany tylko i wyłącznie ograniczeniami w zasobach obliczeniowych. W rzeczywistości, aby otrzymać konkluzywne i rzetelne wyniki, należałoby skorzystać co najmniej ze zbioru `CIFAR-10` oraz znacznie większego modelu kodera, np. `ResNet`. Jednak do celów dydaktycznych skorzystamy z mniejszego zbioru oraz odpowiednio mniejszego kodera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600bfc0-49b1-4d1b-911d-5ff61557ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "DATA_DIR = \"./data\"\n",
    "OUT_DIR = \"./data/output/byol\"\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "NUM_SAMPLES_PER_CLASS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009daf18-03c7-4f79-b0f1-54d376a3aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = VisionDatamodule(\n",
    "    root_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    num_samples_per_class=NUM_SAMPLES_PER_CLASS,\n",
    ")\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202c2ee-eb8c-4763-8262-d24666965a81",
   "metadata": {},
   "source": [
    "# Zadanie 3.1 Implementacja modelu BYOL (2.5 pkt)\n",
    "W poniższej komórce znajduje się częściowa implementacja modelu `BYOL`. Uzupełnij brakujące implementacje funkcji:\n",
    "* `copy_and_freeze_module` (0.5 pkt) - kopiuje parametry sieci oraz \"zamraża\" te skopiowane (ustawia brak liczenia gradientu dla skopiowanych parametrów)\n",
    "* `byol_loss` (1.0 pkt) - funkcja straty modelu BYOL (zgodnie z oryginalną publikacją)\n",
    "  $$ \\mathcal{L}_{\\theta, \\zeta} = \\lVert \\bar{q_{\\theta}}(z_{\\theta}) - \\bar{z'_{\\zeta}} \\rVert_2^2 $$\n",
    "  gdzie $\\bar{q_{\\theta}}(z_{\\theta})$ - wektor wyjściowy z predyktora gałęzi `ONLINE`, $\\bar{z'_{\\zeta}}$ - wektor wyjściowy z projektora gałęzi `TARGET`\n",
    "* `update_target_network` (1.0 pkt) - aktualizuje parametry kodera oraz projektora sieci `TARGET`, ustawia je jako średnia krocząca sieci `ONLINE`\n",
    "\n",
    "Ponadto:\n",
    "* Przeanalizuj dokładnie pozostałe elementy implementacji.\n",
    "* Zastanów się, czy wszystkie parametry modelu podlegają uczeniu?\n",
    "* Zastanów dlaczego w funkcji `forward` dodatkowo liczone jest `q_sym` oraz `z_prim_sym`, czy bez tego model będzie nadal działał poprawnie?\n",
    "* Uruchom uczenie i przeanalizuj wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e1f28-c6a9-4c4c-add7-59301a131f1e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2ab7c5116ec665e33daf019a0709fb",
     "grade": true,
     "grade_id": "byol-implementation",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BYOLModel(SSLBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        tau: float,\n",
    "        out_channels: int = 10,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "\n",
    "        # Initialize online network\n",
    "        self.online_encoder = SmallConvnet()\n",
    "        self.online_projector = MLP(84, 84, 84, plain_last=False)\n",
    "        self.online_predictor = MLP(84, 84, 84, plain_last=True)\n",
    "        self.online_net = nn.Sequential(\n",
    "            self.online_encoder,\n",
    "            self.online_projector,\n",
    "            self.online_predictor,\n",
    "        )\n",
    "\n",
    "        # Initialize target network with frozen weights\n",
    "        self.target_encoder = self.copy_and_freeze_module(self.online_encoder)\n",
    "        self.target_projector = self.copy_and_freeze_module(self.online_projector)\n",
    "        self.target_net = nn.Sequential(self.target_encoder, self.target_projector)\n",
    "\n",
    "        # Initialize augmentations\n",
    "        self.aug_1 = get_default_aug()\n",
    "        self.aug_2 = get_default_aug()\n",
    "\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        t = self.aug_1(x)\n",
    "        t_prim = self.aug_2(x)\n",
    "\n",
    "        q = self.online_net(t)\n",
    "        q_sym = self.online_net(t_prim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_prim = self.target_net(t_prim)\n",
    "            z_prim_sym = self.target_net(t)\n",
    "\n",
    "        q = torch.cat([q, q_sym], dim=0)\n",
    "        z_prim = torch.cat([z_prim, z_prim_sym], dim=0)\n",
    "\n",
    "        return q, z_prim\n",
    "\n",
    "    def forward_repr(self, x: Tensor) -> Tensor:\n",
    "        return self.online_encoder(x)\n",
    "\n",
    "    def training_step(self, batch: Tensor, batch_idx: int) -> Tensor:\n",
    "        x, _ = batch\n",
    "        q, z_prim = self.forward(x)\n",
    "        loss = self.byol_loss(q=q, z_prim=z_prim)\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def byol_loss(self, q: Tensor, z_prim: Tensor) -> Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        super().on_train_epoch_end()\n",
    "        self.update_target_network()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target_network(self) -> None:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def copy_and_freeze_module(model: nn.Module) -> nn.Module:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95924778-012e-416e-b5c8-65c982f9aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"./data/output/byol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100419f-7cd7-44b9-86f0-a058848d714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TAU = 0.99\n",
    "EPOCHS = 200\n",
    "ACCELERATOR = \"cpu\"  # change to CUDA, if want to train on GPU\n",
    "\n",
    "seed_everything(42)\n",
    "model = BYOLModel(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    tau=TAU,\n",
    ")\n",
    "logger = TensorBoardLogger(save_dir=OUT_DIR, default_hp_metric=False)\n",
    "trainer = Trainer(\n",
    "    default_root_dir=OUT_DIR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=logger,\n",
    "    accelerator=ACCELERATOR,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47cbac-cbab-4a3e-a5ae-8bf29c2e1e76",
   "metadata": {},
   "source": [
    "# Zadanie 3.2 Znaczenie projektora (0.5 pkt)\n",
    "* Zmodyfikuj klasę modelu BYOL, tak aby nie zawierał on projektora (zarówno w sieci `ONLINE` jak i `TARGET`) i sprawdzić jak różnią się otrzymane wyniki względem poprzedniego eksperymentu\n",
    "* Zinterpretuj otrzymane wyniki, jaka jest rola projektora w tym modelu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d44b52-43e2-4d22-bfc8-b395692615c0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c322d3652ca77e315c8e5db07c80e2f",
     "grade": true,
     "grade_id": "projector-study",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BYOLWithoutProjectorModel(SSLBase):\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85510165-29da-413d-9aef-a55498816620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "TAU = 0.99\n",
    "EPOCHS = 200\n",
    "ACCELERATOR = \"cpu\"  # change to CUDA, if want to train on GPU\n",
    "\n",
    "seed_everything(42)\n",
    "model = BYOLWithoutProjectorModel(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    tau=TAU,\n",
    ")\n",
    "logger = TensorBoardLogger(save_dir=OUT_DIR, default_hp_metric=False)\n",
    "trainer = Trainer(\n",
    "    default_root_dir=OUT_DIR,\n",
    "    max_epochs=EPOCHS,\n",
    "    logger=logger,\n",
    "    accelerator=ACCELERATOR,\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc724a-947c-44c3-8e15-f62d9481d9e8",
   "metadata": {},
   "source": [
    "# Zadanie 3.3 Badanie parametru `tau` w EMA (1 pkt)\n",
    "* Sprawdź jakie wyniki model osiąga dla różnych wartości parametru `tau`, w szczególności przebadaj wartości krańcowe `(0.0, 1.0)`\n",
    "* Idelanie będzie wykonać 3-5 powtórzeń dla każdej wartości parametru, ale jeśli ograniczają Cię zasoby wystarczy 1\n",
    "* Do wyznaczenia metryk wykorzystaj metodę `trainer.test(...)` i przygotuj wykres na podstawie otrzymanych wyników\n",
    "* Pamiętaj, aby badać pierwszą wersję modelu z projektorem, a z każdym kolejnym uruchominiem uczenia ustawiaj na nowo seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff24967-bed7-400f-9942-c1afbcc2862a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8083733d4b4d4fff895f273d503ccf0",
     "grade": true,
     "grade_id": "tau-study",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TU WPISZ KOD\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

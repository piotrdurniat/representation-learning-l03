{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd6063f",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb5e7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764f35f-b8c1-4802-b0a7-03a7f44b6939",
   "metadata": {},
   "source": [
    "# Noise Contrastive Estimation\n",
    "W poniższym zeszycie zapoznamy się w praktyce z metodą Noise Contrastive Estimation (NCE) [(Gutmann and  Hyvarinen, 2012)](https://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf), w której problem nienadzorowanej estymacji rozkładu prawdopodobieństwa zamienia się na problem klasyfikacji -odróżniania rzeczywistego rozkładu od szumu. W praktyce wykorzystuje się nieco bardziej zaawansowane podejścia, jednak metoda ta stanowi podstawę uczenia kontrastowego. W ramach poniższych zadań stosować będziemy oznaczenia i terminologię z oryginalnej publikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn import datasets\n",
    "from torch import distributions as D\n",
    "import matplotlib.animation as animation\n",
    "from lightning_fabric import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a8cae-80e7-4426-9e21-4de269b71051",
   "metadata": {},
   "source": [
    "## Generowanie zbioru danych\n",
    "NCE zostanie przetestowane w ramach sztucznego, prostego problemu, mianowicie estymacji rozkładu mikstur Gaussowskich. Warto pamiętać , że w NCE:\n",
    "* nie znamy rzeczywistego rozkładu danych (tutaj używamy rozkładu wyłącznie do generowania danych, model nie ma informacji na jego temat),\n",
    "* znamy rozkład szumu.\n",
    "\n",
    "Poniżej znajdują się funkcje implementujące generowanie przykładów rzeczywistych oraz definicja rozkładu stanowiącego szum i generowanie z niego przykładów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(n: int) -> Tensor:\n",
    "    gmm = get_noise_distribution()\n",
    "    samples = gmm.sample((n,))\n",
    "    return samples\n",
    "\n",
    "def get_noise_distribution() -> D.Distribution:\n",
    "    mix = D.Categorical(torch.ones(2,))\n",
    "    comp = D.Independent(D.Normal(torch.tensor([[-2, 0], [-10, 8]], dtype=torch.float), torch.ones(2, 2) * 1.25), 1)\n",
    "    gmm = D.MixtureSameFamily(mix, comp)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e16355415e6bb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x, _ = datasets.make_blobs(\n",
    "    n_samples=1_000, \n",
    "    n_features=2, \n",
    "    centers=[[-2, 8], [-10, 0]], \n",
    "    cluster_std=1.05,\n",
    "    random_state=42\n",
    ")\n",
    "noise_samples = sample_noise(1_000)\n",
    "ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], label=\"real $\\\\boldsymbol{x}_t$\")\n",
    "sns.scatterplot(x=noise_samples[:, 0], y=noise_samples[:, 1], ax=ax, color='gray', label=\"noise  $\\\\boldsymbol{y}_t$\")\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Type\")\n",
    "ax.set(title=\"Dataset samples\", xlabel=\"\", ylabel=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66110325e69d1787",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Zadanie 1.1 Implementacja NCE (1.5 pkt)\n",
    "Poniżej znajduje się klasa implementuja metodę `NCE`. Do estymacji rozkładu wykorzystamy pojedyńczą wartswę liniową, która odpowiedzialna będzie za predykcję, czy dana próbka pochodzi z rozkładu rzeczywistego czy z szumu.\n",
    "Predykcja prawdopodobieństwa, że dana próbka została wygenerowana z rzeczywistego rozkładu określona jest wzorem:\n",
    "$$  P(C=1 | \\boldsymbol{u};\\boldsymbol{\\theta}) = \\frac{p_m(\\boldsymbol{u};\\boldsymbol{\\theta})}{p_m(\\boldsymbol{u};\\boldsymbol{\\theta}) + vp_n(\\boldsymbol{u})},$$\n",
    "gdzie $p_m$ - estymowane przez model prawdopodobieństwo, $p_n$ - prawdopodobieństwo szumu, $v$ - stosunek przykładów negatywnych do pozytywnych, $\\boldsymbol{u}$ - dane wejściowe wejściowymi (szum lub dane rzeczywiste), $\\boldsymbol{\\theta}$ - parametry modelu.\n",
    "\n",
    "Przyjmijmy teraz $P(C=1 | \\boldsymbol{u};\\boldsymbol{\\theta}) = h(\\boldsymbol{u};\\boldsymbol{\\theta})$, a następnie zdefiniujmy stosunek log-prawdopodobieństw jako: \n",
    "$$G(\\boldsymbol{u};\\boldsymbol{\\theta}) = \\ln{p_m(\\boldsymbol{u};\\boldsymbol{\\theta})} - \\ln{p_n(\\boldsymbol{u})}$$\n",
    "Dokonując odpowiednich przekształceń, otrzymamy wyjście modelu jako:\n",
    "$$ h(\\boldsymbol{u};\\boldsymbol{\\theta}) = \\frac{1}{1+v\\exp(-G(\\boldsymbol{u};\\boldsymbol{\\theta}))} $$\n",
    "\n",
    "Aby wytrenować model estymujący rozkład należy skorzystać ze standardowej dla klasyfikacji binarnej funkcji straty:\n",
    "$$ l(\\boldsymbol{\\theta}) = \\sum_{t=1}^{Td}\\ln{h(\\boldsymbol{x}_t; \\boldsymbol{\\theta}}) + \\sum_{t=1}^{Tn}\\ln{(1-h(\\boldsymbol{y}_t; \\boldsymbol{\\theta}})),$$\n",
    "gdzie $T_d$ - liczba przykładów pozytywnych (liczba przykładów z danych rzeczywistych), $T_n$ - liczba przykładów negatywnych (liczba przykładów danych losowanych z szumu), $\\boldsymbol{x}_t$ - przykład pozytywny, $\\boldsymbol{y}_t$ - przykład negatywny (szum).\n",
    "\n",
    "Zaimplementuj poniższe funkcje:\n",
    "1. `log_prob_diff` (0.5 pkt) - oblicza stosunek log-prawdopodobieństw, tj. funkcje $G(\\boldsymbol{u};\\boldsymbol{\\theta})$\n",
    "2. `loss` (0.5 pkt) - oblicza wartość funkcji straty $l(\\boldsymbol{\\theta})$\n",
    "3. `predict_proba` (0.5 pkt) - oblicza wartość prawdopodobieństwa przynależności do rozkładu danych rzeczywistych, czyli $P(C=1 | \\boldsymbol{u};\\boldsymbol{\\theta})$ lub $ h(\\boldsymbol{u};\\boldsymbol{\\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4341e20384af8af",
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99405db5a0f73ff5cf1c1ba0b1fe79b5",
     "grade": true,
     "grade_id": "nce-implementation",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "class SimpleNCE(nn.Module):\n",
    "    \"\"\"Implementation of NCE method.\n",
    "    Reference: http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, neg_pos_ratio: float):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.v = neg_pos_ratio\n",
    "\n",
    "        self.noise_dist = get_noise_distribution()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # prepare noise samples\n",
    "        num_noise_samples = int(x.size(0) * self.v)\n",
    "        noise_samples = self.noise_dist.sample((num_noise_samples,))\n",
    "\n",
    "        g_data = self.log_prob_diff(x)\n",
    "        g_noise = self.log_prob_diff(noise_samples)\n",
    "\n",
    "        h_data = self.sigm(g_data) + EPSILON\n",
    "        h_noise = self.sigm(g_noise)\n",
    "        \n",
    "        return self.loss(h_data, h_noise)\n",
    "\n",
    "    def log_prob_diff(self, u: Tensor) -> Tensor:\n",
    "        \"\"\"Computes log_prob_diff of data being real (estimated by model) or noise (analytically from distribution).\"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def loss(self, h_data: Tensor, h_noise: Tensor) -> Tensor:\n",
    "        \"\"\"Computes NCE loss, i.e., binary cross entropy\"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_proba(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Predicts probability of data being from real distribution.\"\"\"\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def sigm(self, u: Tensor) -> Tensor:\n",
    "        \"\"\"Modified sigmoid (accounts for neg_pos_ratio).\"\"\"\n",
    "        return 1 / (1 + self.v * torch.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a505c05f5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_on_grid(\n",
    "    model: nn.Module, \n",
    "    x_range: tuple[int, int], \n",
    "    y_range: tuple[int, int], \n",
    "    resolution: int = 100,\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Generates grid of points which are then evaluated against estimated model.\"\"\"\n",
    "    grid_x1, grid_x2 = torch.meshgrid(\n",
    "        torch.linspace(x_range[0], x_range[1], resolution),\n",
    "        torch.linspace(y_range[0], y_range[1], resolution), \n",
    "        indexing=\"ij\",\n",
    "    )\n",
    "    grid = torch.stack([grid_x1.flatten(), grid_x2.flatten()], dim=1)\n",
    "    proba = model.predict_proba(grid).reshape(resolution, resolution)\n",
    "    return grid_x1, grid_x2, proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab72ce-d172-4f8b-85c1-74d352492c3b",
   "metadata": {},
   "source": [
    "### Estymacja (uczenie)\n",
    "1. Dokładnie zapoznaj się z procedurą uczenia\n",
    "2. Uruchom uczenie i zaobserwuj jego przebieg na wykresie wartości funkcji straty.\n",
    "3. **Uuchom trening kilka razy, zmieniając wartość parametru `POS_NEG_RATIO`, co zaobserwowałeś? (bądź gotowy odpowiedzieć na to pytanie na zajęciach)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "POS_NEG_RATIO = 100\n",
    "\n",
    "xs = torch.tensor(x, dtype=torch.float)\n",
    "loader = DataLoader(TensorDataset(xs), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "seed_everything(42)\n",
    "model = SimpleNCE(in_dim=2, neg_pos_ratio=POS_NEG_RATIO)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "losses = []\n",
    "\n",
    "x_range = (x[:, 0].min(), x[:, 0].max())\n",
    "y_range = (x[:, 1].min(), x[:, 1].max())\n",
    "eval_snapshots = []\n",
    "\n",
    "model.eval()\n",
    "eval_res = eval_on_grid(\n",
    "    model,\n",
    "    x_range=x_range,\n",
    "    y_range=y_range,\n",
    ")\n",
    "eval_snapshots.append(eval_res)\n",
    "\n",
    "with trange(EPOCHS) as pbar:\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        for x_batch, *_ in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            eval_res = eval_on_grid(\n",
    "                model,\n",
    "                x_range=x_range,\n",
    "                y_range=y_range,\n",
    "            )\n",
    "            eval_snapshots.append(eval_res)\n",
    "\n",
    "ax = sns.lineplot(x=range(len(losses)), y=losses)\n",
    "ax.set(title=\"Loss\", xlabel=\"Step\", ylabel=\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27942a50-b288-46a9-a859-e99b300d6a00",
   "metadata": {},
   "source": [
    "### Wizualizacja estymowanego rozkładu\n",
    "Wykonaj poniższą komórkę i sprawdź, jakie prawdopodobieństwa model przypisał w poszczególnych obszarach. Co to oznacza, zinterpretuj wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19409692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "num_steps = len(eval_snapshots)\n",
    "col_wrap = 2\n",
    "num_rows = math.ceil(num_steps / col_wrap)  \n",
    "fig, axes = plt.subplots(num_rows, col_wrap, figsize=(5*col_wrap, 5*num_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_steps):\n",
    "    grid_x, grid_y, proba = eval_snapshots[i]\n",
    "    grid_x, grid_y, proba = grid_x.numpy(), grid_y.numpy(), proba.numpy()\n",
    "    cont = axes[i].contourf(grid_x, grid_y, proba)\n",
    "    axes[i].set(title=f\"Epoch {i*5}\")\n",
    "\n",
    "fig.colorbar(cont, ax=axes.ravel().tolist())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
